{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "%matplotlib inline\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "PATH_TO_FROZEN_GRAPH = r'Capstone_project_2\\inference_graph\\frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = r'Capstone_project_2\\training\\labelmap.pbtxt'\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "    # Run inference\n",
    "    output_dict = sess.run(tensor_dict,\n",
    "                            feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "    output_dict['detection_classes'] = output_dict[\n",
    "        'detection_classes'][0].astype(np.uint8)\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "    if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_file_name = 'no_flash_meter_Copy/m30.jpg'  #m3 , m4 , m6 , m7 ,m8 ,m9 , m13 , m14 , m16 , m23 , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "#os.chdir(r'C:\\Users\\Suyash\\no_flash_meter_Copy')\n",
    "import cv2\n",
    "with detection_graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "                # Get handles to input and output tensors\n",
    "                ops = tf.get_default_graph().get_operations()\n",
    "                all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "                tensor_dict = {}\n",
    "                for key in [\n",
    "                  'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                  'detection_classes', 'detection_masks'\n",
    "                ]:\n",
    "                    tensor_name = key + ':0'\n",
    "                    if tensor_name in all_tensor_names:\n",
    "                        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                      tensor_name)\n",
    "                image_np = cv2.imread(input_image_file_name) #m6\n",
    "                # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "                image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "                # Actual detection.\n",
    "                output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "                # Visualization of the results of a detection.\n",
    "                vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                        image_np,\n",
    "                        output_dict['detection_boxes'],\n",
    "                        output_dict['detection_classes'],\n",
    "                        output_dict['detection_scores'],\n",
    "                        category_index,\n",
    "                        instance_masks=output_dict.get('detection_masks'),\n",
    "                        use_normalized_coordinates=True,\n",
    "                        line_thickness=0)\n",
    "                cv2.imshow('object_detection', cv2.resize(image_np, (800, 600)))\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This box is gonna get used [0.32117024 0.07294898 0.44991475 0.8186369 ] 1\n"
     ]
    }
   ],
   "source": [
    "coordinate = []\n",
    "# This is the way I'm getting my coordinates\n",
    "boxes = output_dict['detection_boxes']\n",
    "# get all boxes from an array\n",
    "max_boxes_to_draw = boxes.shape[0]\n",
    "# get scores to get a threshold\n",
    "scores = output_dict['detection_scores']\n",
    "# this is set as a default but feel free to adjust it to your needs\n",
    "min_score_thresh=.5\n",
    "# iterate over all objects found\n",
    "for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    # \n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "        # boxes[i] is the box which will be drawn\n",
    "        class_name = category_index[output_dict['detection_classes'][i]]['name']\n",
    "        print (\"This box is gonna get used\", boxes[i], output_dict['detection_classes'][i])\n",
    "        coordinate=boxes[i]\n",
    "        \n",
    "\n",
    "height, width, channel = image_np.shape\n",
    "ymin= int((coordinate[0])*height)\n",
    "xmin= int((coordinate[1])*width)\n",
    "ymax = int((coordinate[2])*height)\n",
    "\n",
    "xmax = int((coordinate[3])*width)\n",
    "\n",
    "\n",
    "crop_img = image_np[ymin:ymax, xmin:xmax]\n",
    "crop_img = cv2.resize(crop_img,(600,100))\n",
    "cv2.imshow(\"cropped\", crop_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "gray = cv2.cvtColor(crop_img,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('gray',gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def crop_imgage(img, scale=1.0):\n",
    "    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n",
    "    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n",
    "    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n",
    "    return img_cropped\n",
    "\n",
    "crop_img = crop_imgage(crop_img, 0.90)\n",
    "\n",
    "cv2.imshow('bilateral',crop_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#blur = cv2.GaussianBlur(gray,(5,5),1)\n",
    "\n",
    "blur = cv2.fastNlMeansDenoisingColored(crop_img,None,10,10,7,3)\n",
    "blur = cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)\n",
    "#blur= clahe.apply(gray)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,1,1,17,3)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,3))\n",
    "dst = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "dst = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "cannny =cv2.Canny(blur, 10, 25)\n",
    "\n",
    "cv2.imshow('thresh',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "threshold2 = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
    "threshold3 = cv2.morphologyEx(threshold2, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
    "\n",
    "cv2.imshow('thresh1',threshold3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "threshold3 = cv2.morphologyEx(dst, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
    "#threshold2 = cv2.dilate(threshold2, np.ones((5,1), np.uint8), iterations=1)\n",
    "height, width = threshold2.shape[:2]\n",
    "threshold2 = threshold2[5:height,5:width]\n",
    "\n",
    "\n",
    "cv2.imshow('thresh_3',threshold3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "dilation = cv2.dilate(threshold3,kernel,iterations = 2)\n",
    "\n",
    "cv2.imshow('dialate',dilation)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "im = dilation.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 66, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 44, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 22, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 22, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 22528)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               2883712   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 3,024,362\n",
      "Trainable params: 3,024,106\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "model = load_model('model_2.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Digits present is 8\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.perspective import four_point_transform\n",
    "from imutils import contours\n",
    "import imutils\n",
    "import cv2\n",
    "cnts = cv2.findContours(im.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "digitCnts = []\n",
    " \n",
    "# loop over the digit area candidates\n",
    "for c in cnts:\n",
    "    # compute the bounding box of the contour\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    " \n",
    "    # if the contour is sufficiently large, it must be a digit\n",
    "    if w >= 0 and (h >= 35):\n",
    "        digitCnts.append(c)\n",
    "\n",
    "print(\"Number of Digits present is \"+str(len(digitCnts)))\n",
    "img_rows, img_cols = 48,68\n",
    "\n",
    "digitCnts = contours.sort_contours(digitCnts,method=\"left-to-right\")[0]\n",
    "digits = []\n",
    "op_dig = []\n",
    "#dim = (48,68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 46)\n",
      "[0]\n",
      "(59, 46)\n",
      "[0]\n",
      "(58, 45)\n",
      "[8]\n",
      "(58, 46)\n",
      "[8]\n",
      "(54, 43)\n",
      "[4]\n",
      "(59, 39)\n",
      "[5]\n",
      "(56, 34)\n",
      "[7]\n",
      "(55, 34)\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(digitCnts)):\n",
    "    mask = np.zeros_like(im) # Create mask where white is what we want, black otherwise\n",
    "    cv2.drawContours(mask, digitCnts, i, 255, -1) # Draw filled contour in mask\n",
    "    out = np.zeros_like(im) # Extract out the object and place into output image\n",
    "    out[mask == 255] = im[mask == 255]\n",
    "    (y, x) = np.where(mask == 255)\n",
    "    (topy, topx) = (np.min(y), np.min(x))\n",
    "    (bottomy, bottomx) = (np.max(y), np.max(x))\n",
    "    out = out[topy:bottomy+1, topx:bottomx+1]\n",
    "    #out = cv2.erode(out, kernel, iterations=1) \n",
    "    #os.chdir(r'digits')\n",
    "    print(out.shape)\n",
    "    cv2.imshow('Output', out)\n",
    "    output = out.copy()\n",
    "    im_1 = cv2.resize(output,  (img_rows, img_cols)) \n",
    "    im_1.reshape((img_rows,img_cols))\n",
    "    #print(im_1.shape) # (28,28)\n",
    "    batch = np.expand_dims(im_1,axis=0)\n",
    "    batch = np.expand_dims(batch,axis=3)\n",
    "    pr = model.predict_classes(im_1.reshape((1, 68, 48,1)))\n",
    "    print(pr)\n",
    "    s = pr[0]\n",
    "    op_dig.append(s)\n",
    "    pro = model.predict_proba(im_1.reshape((1, 68, 48,1)))\n",
    "    #print(pro)\n",
    "    #print(pro[0,s])\n",
    "    ##cv2.imwrite(\"template {0}.jpg\".format(i),out)\n",
    "    #print(pytesseract.image_to_string(out, lang=\"letsgodigital\", config=\"--psm 10 -c tessedit_char_whitelist=.0123456789\"))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "#os.chdir(r'C:\\Users\\Suyash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(op_dig[0]!=0)and (len(digitCnts)==9):\n",
    "    op_dig.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 8, 8, 4, 5, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(op_dig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00884577"
     ]
    }
   ],
   "source": [
    "for i in op_dig: \n",
    "    print(i, end=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('object_detection', cv2.resize(image_np, (800, 600)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
